<!DOCTYPE html><html lang="en-gb"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>DH 150 Mini-Assignment 1</title><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://nishamcnealis.github.io/DH150_project/"><link rel="alternate" type="application/atom+xml" href="https://nishamcnealis.github.io/DH150_project/feed.xml"><link rel="alternate" type="application/json" href="https://nishamcnealis.github.io/DH150_project/feed.json"><meta property="og:title" content="DH 150 Mini-Assignment 1"><meta property="og:site_name" content="DH 150 Mini-Assignment 1"><meta property="og:description" content=""><meta property="og:url" content="https://nishamcnealis.github.io/DH150_project/"><meta property="og:type" content="website"><link rel="shortcut icon" href="https://nishamcnealis.github.io/DH150_project/media/website/Screen-Shot-2022-10-15-at-5.43.53-PM-2.png" type="image/x-icon"><link rel="stylesheet" href="https://nishamcnealis.github.io/DH150_project/assets/css/style.css?v=a1172f7828d96554eca4f84bc4b2d7dc"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Organization","name":"DH 150 Mini-Assignment 1","url":"https://nishamcnealis.github.io/DH150_project/"}</script></head><body><div class="site-container"><header class="top" id="js-header"><a class="logo" href="https://nishamcnealis.github.io/DH150_project/">DH 150 Mini-Assignment 1</a><nav class="navbar js-navbar"><button class="navbar__toggle js-toggle" aria-label="Menu" aria-haspopup="true" aria-expanded="false"><span class="navbar__toggle-box"><span class="navbar__toggle-inner">Menu</span></span></button><ul class="navbar__menu"><li><a href="https://nishamcnealis.github.io/DH150_project/alexa-play-an-exploration-of-ai-assistants-from-a-feminist-standpoint-by-nisha-mcnealis.html" target="_self">Home</a></li><li><a href="https://nishamcnealis.github.io/DH150_project/about.html" target="_self">About</a></li></ul></nav></header><main><article class="post"><div class="hero"><figure class="hero__image hero__image--overlay"><img src="https://nishamcnealis.github.io/DH150_project/media/posts/1/Screen-Shot-2022-10-18-at-9.00.51-AM.png" srcset="https://nishamcnealis.github.io/DH150_project/media/posts/1/responsive/Screen-Shot-2022-10-18-at-9.00.51-AM-xs.png 300w, https://nishamcnealis.github.io/DH150_project/media/posts/1/responsive/Screen-Shot-2022-10-18-at-9.00.51-AM-sm.png 480w, https://nishamcnealis.github.io/DH150_project/media/posts/1/responsive/Screen-Shot-2022-10-18-at-9.00.51-AM-md.png 768w, https://nishamcnealis.github.io/DH150_project/media/posts/1/responsive/Screen-Shot-2022-10-18-at-9.00.51-AM-lg.png 1024w, https://nishamcnealis.github.io/DH150_project/media/posts/1/responsive/Screen-Shot-2022-10-18-at-9.00.51-AM-xl.png 1360w, https://nishamcnealis.github.io/DH150_project/media/posts/1/responsive/Screen-Shot-2022-10-18-at-9.00.51-AM-2xl.png 1600w" sizes="(max-width: 1600px) 100vw, 1600px" loading="eager" height="726" width="1438" alt=""></figure><header class="hero__content"><div class="wrapper"><h1>An Exploration of AI Assistants &amp; Gender</h1></div></header></div><div class="wrapper post__entry"><p><span style="font-weight: 400;">Voice assistants (VAs) are quickly becoming a predominant feature of everything from our smartphones to our cars to our kitchen devices. We're increasingly reliant on them for communication with others, scheduling, home maintenance, and even companionship. By 2024, digital voice assistants are expected to outnumber people!</span></p><p><span style="font-weight: 400;">As these technologies become more and more advanced, we should think critically about their societal impacts. In this site, we'll explore how developers’ efforts to feminize, and by extension, personify, virtual assistants will continue to create a less equitable, more dangerous world for women.</span></p><h2>Background</h2><p><span style="font-weight: 400;">VAs are perceived as unmistakably female.</span></p><p><span style="font-weight: 400;">Siri, Alexa, Google Assistant, and Cortana were all initially released with default female voices. Apple recently made the switch to a gender-neutral default voice, but it’s hard to argue that “Siri” is genderless when the name itself is Norse for “beautiful woman who leads you to victory.”</span></p><figure class="post__image post__image--right"><figure class="post__image post__image--center"><img loading="lazy" style="margin-bottom: 1.37143rem; margin-left: 1.37143rem; outline: 3px solid rgba(var(--color-primary-rgb), 0.55)  !important;" src="https://api.time.com/wp-content/uploads/2014/04/cortana-halo-41.jpg?quality=85&amp;w=1000" data-is-external-image="true" alt="Image of Cortana from Time Magazine" width="200" height="300"></figure><figcaption>Image of Cortana<br>from Time Maganzine</figcaption></figure><p><span style="font-weight: 400;">If you think th</span><span style="font-weight: 400;">at’s bad, “Cortana” is a reference to a highly sexualized female character in the video game Halo.</span></p><p><span style="font-weight: 400;">Wired reporter Jessi Hempel notes, “we assign female pronouns to them, and in turn, they fold feminine turns of phrase into their robotic and occasionally inane answers to our requests.”</span></p><p><span style="font-weight: 400;">This pattern is not exclusive to AI assistants — we also hear female voices in everything from GPS navigation to voicemail systems to public transport announcements. There are several reasons behind the feminization of computerized systems. Check out these slides to learn more.</span></p><div class="post__iframe"><iframe loading="lazy" width="862" height="511" style="width: 862px; height: 511px;" src="https://docs.google.com/presentation/d/e/2PACX-1vSp0Z0ZqVwjTHxCd-Cw9i22FxccDfHeqqHoVjQVa5JaXA9C-eHy6WrzOHsXP5J3CiCwz5kmpRvf5L1N/embed?start=false&amp;loop=false&amp;delayms=3000" frameborder="0" allowfullscreen="allowfullscreen" mozallowfullscreen="mozallowfullscreen" webkitallowfullscreen="webkitallowfullscreen"></iframe></div><h2>Case Studies</h2><p>Try out this interactive activity to learn about two chatbots that are especially relevant to this discussion.</p><figure class="post__image"><img loading="lazy" src="https://nishamcnealis.github.io/DH150_project/media/posts/1/Screen-Shot-2022-10-18-at-8.54.18-AM.png" sizes="100vw" srcset="https://nishamcnealis.github.io/DH150_project/media/posts/1/responsive/Screen-Shot-2022-10-18-at-8.54.18-AM-xs.png 300w, https://nishamcnealis.github.io/DH150_project/media/posts/1/responsive/Screen-Shot-2022-10-18-at-8.54.18-AM-sm.png 480w, https://nishamcnealis.github.io/DH150_project/media/posts/1/responsive/Screen-Shot-2022-10-18-at-8.54.18-AM-md.png 768w, https://nishamcnealis.github.io/DH150_project/media/posts/1/responsive/Screen-Shot-2022-10-18-at-8.54.18-AM-lg.png 1024w, https://nishamcnealis.github.io/DH150_project/media/posts/1/responsive/Screen-Shot-2022-10-18-at-8.54.18-AM-xl.png 1360w, https://nishamcnealis.github.io/DH150_project/media/posts/1/responsive/Screen-Shot-2022-10-18-at-8.54.18-AM-2xl.png 1600w" alt="" width="1136" height="266"></figure><p class="align-center"><button style="all: unset; font-family: Helvetica,Arial,sans-serif; display: inline-block; max-width: 100%; white-space: nowrap; overflow: hidden; text-overflow: ellipsis; background-color: #a87db7; color: #ffffff; font-size: 20px; border-radius: 25px; padding: 0 33px; font-weight: bold; height: 50px; cursor: pointer; line-height: 50px; text-align: center; margin: 0; text-decoration: none;" data-tf-popup="LTmjbiRH" data-tf-size="100">Try me!</button><script src="//embed.typeform.com/next/embed.js"></script></p><hr><h2>Ethical Concerns</h2><p><span style="font-weight: 400;">VA creators have argued that their products should not be considered human-like at all. Amazon’s branding guidelines claim “Alexa isn’t a person, but has a persona – Amazon personifies Alexa as an artificial intelligence (AI) and not as a person with a physical body or a gender identity.” And Apple’s style guide specifies “Do not refer to Siri with pronouns such as ‘she,’ ‘him,’ or ‘her.’ Depending on language support, Siri may offer a male or female voice, or both.”</span></p><p><span style="font-weight: 400;">In practice, however, voice assistants are frequently anthropomorphized and sexualized. An analysis of Amazon.com user reviews of the Echo device, which uses Alexa, found that more than 30% participated in some personification of the product. 13.6% used the name “Alexa” with pronouns like “she” and “her” and over 2,000 reviewers compared Alexa to a companion, girlfriend, wife, secretary, or family member (Gao 2018). In a 2015 survey of 12,000 digital assistant users, 39% claimed that they would fall in love with a voice assistant if it could love them back (API.ai, 2015).</span></p><p><span style="font-weight: 400;">The widespread adoption of virtual assistants, particularly female-presenting ones, raises a variety of ethical concerns.</span></p><h4 class="msg--highlight"><span style="font-weight: 400;">1. Manufacturing Human Connection</span></h4><p><span style="font-weight: 400;">According to a 2018 study, users can be aware that AI assistants are not sentient and still implicitly view them as humanoid — people responded with signs of stress when asked to switch off a robot with a computerized voice that was begging them not to. The researchers wrote, “due to their social nature, people will rather make the mistake of treating something falsely as human than treating something falsely as non-human.” </span>  </p><div class="gallery-wrapper"><div class="gallery" data-is-empty="false" data-translation="Add images" data-columns="3"><figure class="gallery__item"><a href="https://nishamcnealis.github.io/DH150_project/media/posts/1/gallery/tumblr_oftwwwYhHD1vvi3bvo2_250.gif" data-size="245x150"><img loading="lazy" src="https://nishamcnealis.github.io/DH150_project/media/posts/1/gallery/tumblr_oftwwwYhHD1vvi3bvo2_250.gif" alt=""></a></figure><figure class="gallery__item"><a href="https://nishamcnealis.github.io/DH150_project/media/posts/1/gallery/anigif_sub-buzz-672-1567719102-2-2.gif" data-size="268x177"><img loading="lazy" src="https://nishamcnealis.github.io/DH150_project/media/posts/1/gallery/anigif_sub-buzz-672-1567719102-2-2.gif" alt=""></a></figure><figure class="gallery__item"><a href="https://nishamcnealis.github.io/DH150_project/media/posts/1/gallery/smile-delight.gif" data-size="498x278"><img loading="lazy" src="https://nishamcnealis.github.io/DH150_project/media/posts/1/gallery/smile-delight.gif" alt=""></a></figure></div></div><p class="msg--info msg"><em>Watchers of The Good Place might recognize this running joke in which human characters struggle to reboot a humanoid personal assistant named Janet.</em></p><p><span style="font-weight: 400;">These results are exacerbated when gender comes into play. A 2021 meta-analysis of four studies found that “using implicit, subtle, and blatant scales of humanness, our results consistently show that women (Studies 1A and 1B), female bots (Studies 2 and 3), and female chatbots (Study 4) are perceived as more human than their male counterparts when compared with non-human entities” (Borau, 2021). Women are more often socialized to be loving, nurturing, and kind, while men are raised to be forceful, assertive, and competent. The researchers theorize that “because warmth and experience (but not competence) are seen as fundamental qualities to be a full human but are lacking in machines, we argue that people prefer female bots because they are perceived as more human than male bots [...] These results highlight the ethical quandary faced by AI designers and policymakers: Women are said to be transformed into objects in AI, but injecting women's humanity into AI objects makes these objects seem more human and acceptable” (Borau, 2021).</span></p><h4 class="msg--highlight"><span style="font-weight: 400;">2. Perpetuation of Misogyny</span></h4><p><span style="font-weight: 400;">It might seem dystopian to imagine a world in which robots meet all our social needs, but virtual assistants might reach this point sooner than we expect. Sites like Invisible Girlfriend and games like LovePlus already simulate relationships online. Are virtual partners healthy, and could they impede our ability to form genuine human connections? Johanna Blakley of the Norman Lear Centre points out that “humans have always used games, play and story time to create simulations of important life experiences: it gives us a chance to practice and to vicariously experience new and strange things in a relatively safe environment.” </span></p><p><span style="font-weight: 400;">But what if these “new and strange things” are offensive, immoral, or illegal? This is where the feminization of voice assistants becomes especially relevant. A United Nations report explained, “the assistant holds no power of agency beyond what the commander asks of it. It honours commands and responds to queries regardless of their tone or hostility. In many communities, this reinforces commonly held gender biases that women are subservient and tolerant of poor treatment.” Implicit sexism toward voice assistants may not be confined to a “safe environment” — it may instead encourage sexist attitudes toward real women.</span></p><figure class="post__image align-center"><img loading="lazy" src="https://nishamcnealis.github.io/DH150_project/media/posts/1/Screen-Shot-2022-10-18-at-8.38.33-AM.png" sizes="100vw" srcset="https://nishamcnealis.github.io/DH150_project/media/posts/1/responsive/Screen-Shot-2022-10-18-at-8.38.33-AM-xs.png 300w, https://nishamcnealis.github.io/DH150_project/media/posts/1/responsive/Screen-Shot-2022-10-18-at-8.38.33-AM-sm.png 480w, https://nishamcnealis.github.io/DH150_project/media/posts/1/responsive/Screen-Shot-2022-10-18-at-8.38.33-AM-md.png 768w, https://nishamcnealis.github.io/DH150_project/media/posts/1/responsive/Screen-Shot-2022-10-18-at-8.38.33-AM-lg.png 1024w, https://nishamcnealis.github.io/DH150_project/media/posts/1/responsive/Screen-Shot-2022-10-18-at-8.38.33-AM-xl.png 1360w, https://nishamcnealis.github.io/DH150_project/media/posts/1/responsive/Screen-Shot-2022-10-18-at-8.38.33-AM-2xl.png 1600w" alt="" width="676" height="379"></figure><p><span style="font-weight: 400;">And it’s not just adults who are using thes</span><span style="font-weight: 400;">e devices — products like Echo and Google Home are intended to provide services for the entire household. This means that by design, developing children may observe or participate in the mistreatment of female AI assistants. Guardian reporter Amelia Hill writes, “The rapid rise in voice assistants [could] have a long-term impact on children’s social and cognitive development, specifically their empathy, compassion and critical thinking skills.”</span></p><p><span style="font-weight: 400;">According to writer and technologist Gideon Rosenblatt, “as digital assistants become increasingly human-like, it’s not unreasonable to suspect that toxic behavior habits with these relationships could spread to our relationships with people. This is particularly true when there are absolutely no negative consequences for our abusive behavior.”</span></p><h4 class="msg--highlight"><span style="font-weight: 400;">3. Exacerbated Impacts on Marginalized Groups</span></h4><p><span style="font-weight: 400;">Each of these issues can and should be analyzed through an intersectional lens.</span></p><figure class="post__image align-center"><img loading="lazy" src="https://nishamcnealis.github.io/DH150_project/media/posts/1/Screen-Shot-2022-10-18-at-8.55.54-AM.png" sizes="100vw" srcset="https://nishamcnealis.github.io/DH150_project/media/posts/1/responsive/Screen-Shot-2022-10-18-at-8.55.54-AM-xs.png 300w, https://nishamcnealis.github.io/DH150_project/media/posts/1/responsive/Screen-Shot-2022-10-18-at-8.55.54-AM-sm.png 480w, https://nishamcnealis.github.io/DH150_project/media/posts/1/responsive/Screen-Shot-2022-10-18-at-8.55.54-AM-md.png 768w, https://nishamcnealis.github.io/DH150_project/media/posts/1/responsive/Screen-Shot-2022-10-18-at-8.55.54-AM-lg.png 1024w, https://nishamcnealis.github.io/DH150_project/media/posts/1/responsive/Screen-Shot-2022-10-18-at-8.55.54-AM-xl.png 1360w, https://nishamcnealis.github.io/DH150_project/media/posts/1/responsive/Screen-Shot-2022-10-18-at-8.55.54-AM-2xl.png 1600w" alt="" width="674" height="379"></figure><p><span style="font-weight: 400;">It is not an accident that our media chooses white women to portray representations of virtual assistants. Plus, white dialects are used for computerized voices in the real world. As cultural theorist Mladen Dolar put it, the vocal norm used by these machines is “an accent which has been declared a non-accent.” And even when voice assistants offer stereotypically male voices in addition to female ones, these options reinforce the gender binary. </span></p></div><footer class="wrapper post__footer"><div class="post__share"></div><div class="post__bio bio"><img src="https://nishamcnealis.github.io/DH150_project/media/website/IMG_7629-2.jpg" loading="lazy" height="1154" width="1149" class="bio__avatar" alt="Nisha McNealis"><div class="bio__info"><h3 class="bio__name"><a href="https://nishamcnealis.github.io/DH150_project/authors/nisha-mcnealis/" rel="author">Nisha McNealis</a></h3><p>Nisha is a fourth-year computer science major at UCLA.</p></div></div></footer></article></main><footer class="footer"><div class="footer__copyright"><p><a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img style="border-width: 0;" src="https://i.creativecommons.org/l/by/4.0/88x31.png" alt="Creative Commons License"></a><br>Alexa, play (“An Exploration of AI Assistants from a Feminist Standpoint”) is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.</p></div><button onclick="backToTopFunction()" id="backToTop" class="footer__bttop" aria-label="Back to top" title="Back to top"><svg><use xlink:href="https://nishamcnealis.github.io/DH150_project/assets/svg/svg-map.svg#toparrow"/></svg></button></footer></div><script defer="defer" src="https://nishamcnealis.github.io/DH150_project/assets/js/scripts.min.js?v=f47c11534595205f20935f0db2a62a85"></script><script>window.publiiThemeMenuConfig={mobileMenuMode:'sidebar',animationSpeed:300,submenuWidth: 'auto',doubleClickTime:500,mobileMenuExpandableSubmenus:true,relatedContainerForOverlayMenuSelector:'.top'};</script><script>var images = document.querySelectorAll('img[loading]');

        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script></body></html>